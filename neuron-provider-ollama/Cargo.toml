[package]
name = "neuron-provider-ollama"
version = "0.2.0"
edition.workspace = true
rust-version.workspace = true
description = "Ollama API client for Rust â€” local LLM inference with NDJSON streaming and tool use. Part of the neuron agent blocks ecosystem."
license.workspace = true
repository.workspace = true
homepage.workspace = true
documentation = "https://docs.rs/neuron-provider-ollama"
keywords = ["ai", "llm", "ollama", "local", "streaming"]
categories = ["science", "api-bindings"]
exclude = ["CLAUDE.md"]

[dependencies]
neuron-types.workspace = true
reqwest.workspace = true
serde.workspace = true
serde_json.workspace = true
futures.workspace = true
tracing.workspace = true
tokio.workspace = true
async-stream.workspace = true
bytes.workspace = true
uuid.workspace = true

[dev-dependencies]
tokio = { workspace = true, features = ["macros", "rt-multi-thread"] }
wiremock.workspace = true
